{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Building a \"fake news\" classifier\n",
    "\n",
    "Anda akan menerapkan dasar-dasar apa yang telah Anda pelajari bersama dengan beberapa *supervised machine learning* untuk membangun detektor **\"fake news/berita palsu\"**. Anda akan mulai dengan mempelajari dasar-dasar *supervised machine learning*, dan kemudian bergerak maju dengan memilih beberapa fitur penting dan menguji ide untuk mengidentifikasi dan mengklasifikasikan artikel berita palsu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying fake news using supervised learning with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is supervised learning?\n",
    "\n",
    "* Bentuk dari machine learning\n",
    "  * Masalah memiliki training data yang telah ditentukan sebelumnya\n",
    "  * Data ini memiliki label (atau hasil) yang model Anda ingin pelajari\n",
    "  * Masalah klasifikasi\n",
    "  * Tujuan: Membuat hipotesis yang baik tentang spesies berdasarkan fitur geometris\n",
    "  \n",
    "<img src=\"datasets/iris-data.png\" width=500px height=500px align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning with NLP\n",
    "\n",
    "* Perlu menggunakan bahasa alih-alih fitur geometris\n",
    "* **scikit-learn**: Powerful open-source library\n",
    "* Bagaimana cara membuat data pembelajaran yang diawasi (*supervised learning*) dari teks?\n",
    "  * Gunakan fitur `bag-of-words` atau `tf-idf` sebagai fitur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Movie Dataset\n",
    "\n",
    "* Goal: Memprediksi genre film berdasarkan ringkasan plot\n",
    "* Fitur Categorikal yang dihasilkan menggunakan preprocessing\n",
    "\n",
    "<img src=\"datasets/imdb-datasets.png\" width=500px height=500px align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning steps\n",
    "\n",
    "* Mengumpulkan dan preprocess data\n",
    "* Tentukan label (Contoh: Genre film)\n",
    "* Membagi data menjadi training dan test sets\n",
    "* Ekstrak fitur dari teks untuk membantu memprediksi label\n",
    "  * Bag-of-words vektor dibangun menjadi scikit-learn\n",
    "* Mengevaluasi model yang sudah di training menggunakan set test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitur apa yang mungkin?\n",
    "\n",
    "Berikut ini merupakan fitur yang mungkin untuk masalah klasifikasi teks:\n",
    "\n",
    "* Jumlah kata dalam dokumen.\n",
    "* Specific named entities.\n",
    "* Language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing\n",
    "\n",
    "Dataset apa yang dibutuhkan untuk pembelajaran yang diawasi?\n",
    "\n",
    "* **training dan testing data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building word count vectors with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting movie genre\n",
    "\n",
    "* Dataset terdiri dari plot film dan genre yang sesuai\n",
    "* Goal: Buat vektor bag-of-word untuk plot film\n",
    "  * Bisakah kita memprediksi genre berdasarkan kata-kata yang digunakan dalam ringkasan plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer with Python\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = ... # Load data into DataFrame\n",
    "y = df['Sci-Fi']\n",
    "X_train, X_test, y_train, y_test = train_test_split( df['plot'], y, test_size=0.33, random_state=53)\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer for text classification\n",
    "\n",
    "Sudah waktunya untuk mulai membangun *text classifier* Anda! [Data](https://s3.amazonaws.com/assets.datacamp.com/production/course_3629/fake_or_real_news.csv) telah dimuat ke dalam DataFrame yang disebut `df`.\n",
    "\n",
    "Dalam latihan ini, Anda akan menggunakan pandas bersama scikit-learn untuk membuat *sparse text vectorizer* yang dapat Anda gunakan untuk melatih dan menguji model sederhana supervised. Untuk memulai, Anda akan menyiapkan `CountVectorizer` dan menyelidiki beberapa fitur-fiturnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/fake_or_real_news.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a series to store the labels: y\n",
    "y = df.label\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], y, test_size=0.33, random_state=53)\n",
    "\n",
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Transform the training data using only the 'text' column values: count_train \n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data using only the 'text' column values: count_test \n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features of the count_vectorizer\n",
    "print(count_vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer for text classification\n",
    "\n",
    "Mirip dengan `CountVectorizer` yang dibuat dalam latihan sebelumnya, Anda akan bekerja membuat vektor tf-idf untuk dokumen Anda. Anda akan mengatur `TfidfVectorizer` dan menyelidiki beberapa fitur-fiturnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
    "\n",
    "# Transform the training data: tfidf_train \n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 10 features\n",
    "print(tfidf_vectorizer.get_feature_names()[:10])\n",
    "\n",
    "# Print the first 5 vectors of the tfidf training data\n",
    "print(tfidf_train.A[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the vectors\n",
    "\n",
    "Untuk mendapatkan ide yang lebih baik tentang cara kerja vektor, Anda akan menyelidikinya dengan mengonversinya menjadi `pandas` DataFrames.\n",
    "\n",
    "Di sini, Anda akan menggunakan struktur data yang sama yang Anda buat dalam dua latihan sebelumnya ( `count_train`, `count_vectorizer`, `tfidf_train`, `tfidf_vectorizer` )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "# Print the head of count_df\n",
    "print(count_df.head())\n",
    "\n",
    "# Print the head of tfidf_df\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference in columns: difference\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference)\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing a classification model with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier\n",
    "\n",
    "* Naive Bayes Model\n",
    "  * Biasa digunakan untuk menguji masalah klasifikasi NLP\n",
    "  * Dasar dalam probability\n",
    "* Mengingat sepotong data tertentu, seberapa besar kemungkinan hasilnya?\n",
    "* Contoh:\n",
    "  * Jika plot memiliki spaceship, seberapa besar kemungkinannya menjadi sci-fi?\n",
    "  * Diberi spaceship dan alien, seberapa mungkin sci-fi sekarang?\n",
    "* Setiap kata dari `CountVectorizer` bertindak sebagai fitur\n",
    "* Naive Bayes: Sederhana dan efektif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.893352462936394"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "pred = nb_classifier.predict(count_test)\n",
    "metrics.accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "```python\n",
    "metrics.confusion_matrix(y_test, pred, labels=[0,1])\n",
    "```\n",
    "\n",
    "<img src=\"datasets/confusion-matrix.png\" width=400px height=400px align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification models\n",
    "\n",
    "Manakah dari model yang paling masuk akal untuk digunakan saat melatih model baru yang diawasi menggunakan data vektor teks?\n",
    "\n",
    "* Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the \"fake news\" model with `CountVectorizer`\n",
    "\n",
    "Sekarang giliran Anda untuk melatih model \"fake news\" menggunakan fitur yang Anda identifikasi dan ekstrak. Dalam latihan pertama ini Anda akan melatih dan menguji model Naive Bayes menggunakan data `CountVectorizer`.\n",
    "\n",
    "Data training dan test telah dibuat, `count_vectorizer`, `count_train`, dan `count_test` telah dihitung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.893352462936394\n",
      "[[ 865  143]\n",
      " [  80 1003]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing the \"fake news\" model with `TfidfVectorizer`\n",
    "\n",
    "Sekarang setelah Anda mengevaluasi model menggunakan `CountVectorizer`, Anda akan melakukan hal yang sama dengan menggunakan `TfidfVectorizer` dengan model Naive Bayes.\n",
    "\n",
    "Data training dan test telah dibuat, `tfidf_vectorizer`, `tfidf_train`, dan `tfidf_test` telah dihitung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8565279770444764\n",
      "[[ 739  269]\n",
      " [  31 1052]]\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes classifier: nb_classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags: pred\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score: score\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(score)\n",
    "\n",
    "# Calculate the confusion matrix: cm\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NLP, complex problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the model\n",
    "\n",
    "* Tweaking alpha levels.\n",
    "* Mencoba model klasifikasi baru.\n",
    "* Training pada dataset yang lebih besar.\n",
    "* Memperbaiki preprocessing teks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tugas Anda dalam latihan ini adalah menguji beberapa level alpha yang berbeda menggunakan vektor `Tfidf` untuk menentukan apakah ada kombinasi yang berkinerja lebih baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.8813964610234337\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.8976566236250598\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.8938307030129125\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.8900047824007652\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.8857006217120995\n",
      "\n",
      "Alpha:  0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8842659014825442\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.874701099952176\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.8703969392635102\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.8660927785748446\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.8589191774270684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Create the list of alphas: alphas\n",
    "alphas = np.arange(0, 1, step=0.1)\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier: nb_classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels: pred\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy: score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting your model\n",
    "\n",
    "Sekarang Anda telah membuat classifier \"fake news\" classifier, Anda akan menyelidiki apa yang telah dipelajarinya. Anda dapat memetakan bobot vektor penting kembali ke kata-kata aktual menggunakan beberapa teknik inspeksi sederhana.\n",
    "\n",
    "Anda memiliki classifier tfidf Naive Bayes yang berkinerja baik tersedia sebagai `nb_classifier`, dan vektor-vektornya sebagai `tfidf_vectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE [(-11.316312804238807, '0000'), (-11.316312804238807, '000035'), (-11.316312804238807, '0001'), (-11.316312804238807, '0001pt'), (-11.316312804238807, '000km'), (-11.316312804238807, '0011'), (-11.316312804238807, '006s'), (-11.316312804238807, '007'), (-11.316312804238807, '007s'), (-11.316312804238807, '008s'), (-11.316312804238807, '0099'), (-11.316312804238807, '00am'), (-11.316312804238807, '00p'), (-11.316312804238807, '00pm'), (-11.316312804238807, '014'), (-11.316312804238807, '015'), (-11.316312804238807, '018'), (-11.316312804238807, '01am'), (-11.316312804238807, '020'), (-11.316312804238807, '023')]\n",
      "REAL [(-7.742481952533027, 'states'), (-7.717550034444668, 'rubio'), (-7.703583809227384, 'voters'), (-7.654774992495461, 'house'), (-7.649398936153309, 'republicans'), (-7.6246184189367, 'bush'), (-7.616556675728881, 'percent'), (-7.545789237823644, 'people'), (-7.516447881078008, 'new'), (-7.448027933291952, 'party'), (-7.411148410203476, 'cruz'), (-7.410910239085596, 'state'), (-7.35748985914622, 'republican'), (-7.33649923948987, 'campaign'), (-7.2854057032685775, 'president'), (-7.2166878130917755, 'sanders'), (-7.108263114902301, 'obama'), (-6.724771332488041, 'clinton'), (-6.5653954389926845, 'said'), (-6.328486029596207, 'trump')]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels: class_labels\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features: feature_names\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "\n",
    "# Print the first class label and the top 20 feat_with_weights entries\n",
    "print(class_labels[0], feat_with_weights[:20])\n",
    "\n",
    "# Print the second class label and the bottom 20 feat_with_weights entries\n",
    "print(class_labels[1], feat_with_weights[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
