{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simple topic identification\n",
    "\n",
    "Bab ini akan memperkenalkan Anda pada identifikasi topik, yang dapat Anda terapkan pada teks apa pun yang Anda jumpai di berbagai studi kasus. Menggunakan model NLP dasar, Anda akan mengidentifikasi topik dari teks berdasarkan frekuensi istilah. Anda akan bereksperimen dan membandingkan dua metode sederhana: **bag-of-words** dan **Tf-idf** menggunakan NLTK dan library baru Gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word counts with bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words\n",
    "\n",
    "* Metode dasar untuk menemukan topik dalam sebuah teks\n",
    "* Harus terlebih dahulu membuat token menggunakan tokenization\n",
    "* ...dan kemudian hitung semua token\n",
    "* Semakin sering kata, semakin penting artinya\n",
    "* Dapat menjadi cara yang bagus untuk menentukan kata-kata penting dalam sebuah teks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words example\n",
    "\n",
    "* Text: `\"The cat is in the box. The cat likes the box. The box is over the cat.\"`\n",
    "* Bag of words (tanda baca stripped):\n",
    "  * \"The\": 3, \"box\": 3\n",
    "  * \"cat\": 3, \"the\": 3\n",
    "  * \"is\": 2\n",
    "  * \"in\": 1, \"likes\": 1, \"over\": 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 3,\n",
       "         'cat': 3,\n",
       "         'is': 2,\n",
       "         'in': 1,\n",
       "         'the': 3,\n",
       "         'box': 3,\n",
       "         '.': 3,\n",
       "         'likes': 1,\n",
       "         'over': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(word_tokenize(\"The cat is in the box. The cat likes the box. The box is over the cat.\"))\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Counter with bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(\"datasets/News articles.zip\", \"r\") as myzip:\n",
    "    with myzip.open(\"News articles/articles.txt\") as myfile:\n",
    "        article = myfile.read()\n",
    "        \n",
    "article = str(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 269), ('the', 257), ('to', 131), ('of', 119), ('.', 114), ('a', 98), ('in', 93), ('and', 79), ('that', 63), ('is', 45)]\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why preprocess?\n",
    "\n",
    "* Membantu menghasilkan data input yang lebih baik\n",
    "  * Saat melakukan pembelajaran mesin atau metode statistik lainnya\n",
    "* Contoh:\n",
    "  * Tokenization untuk membuat bag of words\n",
    "  * Merubah kata-kata menjadi huruf kecil\n",
    "* Lemmatization/Stemming\n",
    "  * Persingkat kata-kata sampai ke akarnya\n",
    "* Menghapus *stop words*, tanda baca, atau token yang tidak diinginkan\n",
    "* Bagus untuk bereksperimen dengan berbagai pendekatan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing example\n",
    "\n",
    "* **Input text**: \n",
    "  * `Cats, dogs and birds are common pets. So are fish.`\n",
    "* **Output tokens**: \n",
    "  * `cat, dog, bird, common, pet, fish`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"The cat is in the box. The cat likes the box. The box is over the cat.\"\n",
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
    "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
    "Counter(no_stops).most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing practice\n",
    "\n",
    "Sekarang, giliran Anda untuk menerapkan teknik yang telah Anda pelajari untuk membantu membersihkan teks untuk hasil NLP yang lebih baik. Anda harus menghapus kata-kata penghenti (*stop words*) dan karakter non-alfabet, lemmatize, dan melakukan *bag-of-words* pada teks Anda yang sudah dibersihkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('robot', 26), ('said', 25), ('population', 21), ('news', 18), ('growth', 16), ('human', 15), ('fake', 15), ('united', 14), ('continue', 13), ('could', 12)]\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is gensim?\n",
    "\n",
    "* Popular open-source NLP library\n",
    "* Uses top academic models to perform complex tasks\n",
    "  * Building document or word vectors\n",
    "  * Performing topic identification and document comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a word vector?\n",
    "\n",
    "<img src=\"datasets/word-vector.png\" width=600px height=600px align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim Example\n",
    "\n",
    "(Source: http://tlfvincent.github.io/2015/10/23/presidential-speech-topics)\n",
    "\n",
    "<img src=\"datasets/gensim.png\" width=600px height=600px align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a gensim dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " 'a': 1,\n",
       " 'about': 2,\n",
       " 'aliens': 3,\n",
       " 'and': 4,\n",
       " 'movie': 5,\n",
       " 'spaceship': 6,\n",
       " 'the': 7,\n",
       " 'was': 8,\n",
       " '!': 9,\n",
       " 'i': 10,\n",
       " 'liked': 11,\n",
       " 'really': 12,\n",
       " ',': 13,\n",
       " 'action': 14,\n",
       " 'awesome': 15,\n",
       " 'boring': 16,\n",
       " 'but': 17,\n",
       " 'characters': 18,\n",
       " 'scenes': 19,\n",
       " 'alien': 20,\n",
       " 'awful': 21,\n",
       " 'films': 22,\n",
       " 'hate': 23,\n",
       " 'cool': 24,\n",
       " 'is': 25,\n",
       " 'space': 26,\n",
       " 'more': 27,\n",
       " 'please': 28}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "my_documents = ['The movie was about a spaceship and aliens.',\n",
    "                'I really liked the movie!'\n",
    "                ,'Awesome action scenes, but boring characters.',\n",
    "                'The movie was awful! I hate alien films.',\n",
    "                'Space is cool! I liked the movie.',\n",
    "                'More space films, please!']\n",
    "\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a gensim corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
       " [(0, 1),\n",
       "  (5, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1)],\n",
       " [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)],\n",
       " [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model `gensim` dapat dengan mudah disimpan, diperbarui, dan digunakan kembali\n",
    "* Dictionary juga dapat diperbarui\n",
    "* Bag-of-words yang lebih canggih dan kaya fitur, ini dapat digunakan dalam latihan di masa mendatang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and querying a corpus with gensim\n",
    "\n",
    "Saatnya untuk menerapkan metode yang Anda pelajari sebelumnya untuk membuat `gensim` dictionary dan corpus pertama Anda!\n",
    "\n",
    "Anda akan menggunakan struktur data ini untuk menyelidiki tren kata dan topik menarik potensial dalam kumpulan dokumen. Untuk memulai, kami telah mengimpor beberapa artikel berantakan tambahan dari Wikipedia, yang diproses lebih dulu dengan mengurangi semua kata, mengurangi token, dan menghapus *stop words* dan tanda baca. Ini kemudian disimpan dalam list token dokumen sebagai `articles`. Anda harus melakukan preprocessing ringan dan kemudian menghasilkan `gensim` dictionary dan corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create name list in zip file .namelist()\n",
    "name_list = ['Wikipedia articles/wiki_text_bug.txt',\n",
    "             'Wikipedia articles/wiki_text_computer.txt',\n",
    "             'Wikipedia articles/wiki_text_crash.txt',\n",
    "             'Wikipedia articles/wiki_text_debugger.txt',\n",
    "             'Wikipedia articles/wiki_text_debugging.txt',\n",
    "             'Wikipedia articles/wiki_text_exception.txt',\n",
    "             'Wikipedia articles/wiki_text_hopper.txt',\n",
    "             'Wikipedia articles/wiki_text_language.txt',\n",
    "             'Wikipedia articles/wiki_text_malware.txt',\n",
    "             'Wikipedia articles/wiki_text_program.txt',\n",
    "             'Wikipedia articles/wiki_text_reversing.txt',\n",
    "             'Wikipedia articles/wiki_text_software.txt']\n",
    "\n",
    "article_raw = []\n",
    "for i in name_list:\n",
    "    zipfile = ZipFile(\"datasets/Wikipedia articles.zip\")\n",
    "    if i in zipfile.namelist():\n",
    "        article_list = str(zipfile.open(i).read())\n",
    "        article_raw.append(article_list)\n",
    "        \n",
    "articles = []\n",
    "for t in article_raw:\n",
    "    # Tokenize the article: tokens\n",
    "    tokens = word_tokenize(t)\n",
    "    # Convert the tokens into lowercase: lower_tokens\n",
    "    lower_tokens = [t.lower() for t in tokens]\n",
    "    # Retain alphabetic words: alpha_only\n",
    "    alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "    # Remove all stop words: no_stops\n",
    "    no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "    \n",
    "    articles.append(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n",
      "[(1, 1), (13, 1), (15, 1), (18, 1), (26, 1), (29, 1), (36, 1), (37, 4), (45, 2), (46, 7)]\n"
     ]
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim bag-of-words\n",
    "\n",
    "Sekarang, Anda akan menggunakan `gensim` corpus dan dictionary baru Anda untuk melihat istilah paling umum per dokumen dan di semua dokumen. Anda dapat menggunakan dictionary Anda untuk mencari istilah. Coba tebak apa topiknya dan jangan ragu untuk menjelajahi lebih banyak dokumen di IPython Shell!\n",
    "\n",
    "Anda memiliki akses ke `dictionary` dan objek `corpus` yang Anda buat pada latihan sebelumnya, serta `defaultdict` Python dan `itertools` untuk membantu pembuatan struktur data menengah untuk analisis.\n",
    "\n",
    "* `defaultdict` memungkinkan kita untuk menginisialisasi dictionary yang akan menetapkan nilai default ke kunci yang tidak ada. Dengan memasok argumen `int`, kami dapat memastikan bahwa kunci yang tidak ada secara otomatis diberi nilai default `0`. Ini membuatnya ideal untuk menyimpan jumlah kata dalam latihan ini.\n",
    "* `itertools.chain.from_iterable()` memungkinkan kita untuk beralih melalui serangkaian urutan seolah-olah mereka adalah satu urutan berkelanjutan. Dengan menggunakan fungsi ini, kita dapat dengan mudah iterate melalui objek `corpus` kami (yang merupakan list of lists).\n",
    "\n",
    "Dokumen kelima dari `corpus` disimpan dalam variabel `doc`, yang telah diurutkan dalam urutan menurun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debugging 30\n",
      "system 18\n",
      "software 16\n",
      "computer 12\n",
      "tools 12\n",
      "computer 551\n",
      "software 403\n",
      "cite 314\n",
      "ref 259\n",
      "code 223\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "    \n",
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is tf-idf?\n",
    "\n",
    "* Frekuensi istilah - frekuensi dokumen terbalik\n",
    "* Memungkinkan Anda menentukan kata-kata terpenting dalam setiap dokumen\n",
    "* Setiap corpus mungkin telah membagikan kata-kata di luar stopwords\n",
    "* Kata-kata ini harus diturunkan bobotnya menjadi penting\n",
    "* Contoh dari astronomi: \"Sky\"\n",
    "* Pastikan kata yang paling umum tidak muncul sebagai kata kunci\n",
    "* Menyimpan dokumen dengan kata-kata khusus yang frekuensi berbobot tinggi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf formula\n",
    "\n",
    "<img src=\"datasets/tf-idf-formula.png\" width=400px height=400px align=left />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.6709183957946449),\n",
       " (2, 0.44727893052976325),\n",
       " (4, 0.117649747733871),\n",
       " (8, 0.4726135548348919),\n",
       " (11, 0.19269987713035017),\n",
       " (12, 0.1490929768432544),\n",
       " (13, 0.0941547797106057),\n",
       " (16, 0.1883095594212114),\n",
       " (18, 0.0745464884216272),\n",
       " (19, 0.05796791435941204)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(corpus)\n",
    "tfidf[corpus[1][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf with Wikipedia\n",
    "\n",
    "Sekarang giliran Anda untuk menentukan istilah signifikan baru untuk corpus Anda dengan menerapkan `gensim`'s tf-idf. Anda lagi akan memiliki akses ke objek dictionary dan corpus yang sama yang Anda buat dalam latihan sebelumnya - `dictionary`, `corpus`, dan `doc`. Akankah tf-idf membuat hasil yang lebih menarik pada level dokumen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.012839474667861298), (13, 0.01621669825834024), (15, 0.02035008587751938), (18, 0.012839474667861298), (26, 0.025678949335722595)]\n",
      "wolf 0.23014517606620988\n",
      "fence 0.1841161408529679\n",
      "debugging 0.15986590374609647\n",
      "acm 0.15407369601433557\n",
      "tron 0.13808710563972595\n"
     ]
    }
   ],
   "source": [
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
