{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spiders\n",
    "\n",
    "Pelajari cara membuat web crawlers dengan scrapy. Scrapy spiders ini akan melakukan proses crawl pada web melalui beberapa halaman, mengikuti tautan untuk scrape setiap halaman secara otomatis sesuai dengan prosedur yang telah kita pelajari di bab-bab sebelumnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your First Spider\n",
    "\n",
    "* Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaving the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCspider( scrapy.Spider ):\n",
    "    \n",
    "    name = 'dc_spider'\n",
    "    \n",
    "    def start_requests( self ):\n",
    "        urls = [ 'https://www.datacamp.com/courses/all' ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request( url = url, callback = self.parse )\n",
    "\n",
    "    def parse( self, response ):\n",
    "        # simple example: write out the html\n",
    "        html_file = 'datasets/DC_courses.html'\n",
    "        with open( html_file, 'wb' ) as fout:\n",
    "            fout.write( response.body )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perlu memiliki fungsi yang disebut `start_requests`\n",
    "* Harus memiliki setidaknya satu fungsi pengurai/parser untuk menangani kode HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Skinny on start_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_requests( self ):\n",
    "    urls = ['https://www.datacamp.com/courses/all']\n",
    "    for url in urls:\n",
    "        yield scrapy.Request( url = url, callback = self.parse )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `scrapy.Request` di sini akan ada dalam variabel response untuk kita\n",
    "* Argumen `url` memberi tahu kami situs mana yang ingin di scrape\n",
    "* Argumen `callback` memberi tahu kita ke mana harus mengirim variabel respons untuk diproses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inheriting the Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your spider class name is: your_spider\n",
      "It seems you have inherited methods from scrapy.Spider -- NICE!\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "    name = \"your_spider\"\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        pass\n",
    "    # parse method\n",
    "    def parse(self, response):\n",
    "        pass\n",
    "\n",
    "def inspect_class(c):\n",
    "    newc = c()\n",
    "    meths = dir(newc)\n",
    "    if 'name' in meths:\n",
    "        print(\"Your spider class name is:\", newc.name)\n",
    "    if 'from_crawler' in meths:\n",
    "        print(\"It seems you have inherited methods from scrapy.Spider -- NICE!\")\n",
    "    else:\n",
    "        print(\"Oh no! It doesn't seem that you are inheriting the methods from scrapy.Spider!!\")\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class(YourSpider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hurl the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The start_requests method yields the following urls:\n",
      "\t- https://www.datacamp.com\n",
      "\t- https://scrapy.org\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    urls = [\"https://www.datacamp.com\", \"https://scrapy.org\"]\n",
    "    for url in urls:\n",
    "      yield url\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "\n",
    "def inspect_class( c ):\n",
    "  newc = c()\n",
    "  meths = dir( newc )\n",
    "  if 'start_requests' in meths:\n",
    "    print( \"The start_requests method yields the following urls:\" )\n",
    "    for u in newc.start_requests():\n",
    "      print(  \"\\t-\", u )\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Requests\n",
    "\n",
    "### Self Referencing is Classy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling start_requests in YourSpider prints out: Hello World!\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    self.print_msg( \"Hello World!\" )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  # print_msg method\n",
    "  def print_msg( self, msg ):\n",
    "    print( \"Calling start_requests in YourSpider prints out:\", msg )\n",
    "\n",
    "def inspect_class( c ):\n",
    "  newc = c()\n",
    "  try:\n",
    "    newc.start_requests()\n",
    "  except:\n",
    "    print( \"Oh No! Something is wrong with the code! Keep trying.\" )\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with Start Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh No! Something is wrong with the code! Keep trying.\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url, YourSpider )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "\n",
    "def inspect_class( c ):\n",
    "  newc = c()\n",
    "  try:\n",
    "    y = list( newc.start_requests() )\n",
    "    first_yield = y[0]\n",
    "    print( \"The url you would scrape is:\", first_yield.url )\n",
    "    cb = first_yield.callback\n",
    "    print( \"The name of the callback method you called is:\", cb.__name__ )\n",
    "  except:\n",
    "    print( \"Oh No! Something is wrong with the code! Keep trying.\" )\n",
    "    \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and Crawl\n",
    "\n",
    "### Pen Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have collected the author names:\n",
      "\t- Jonathan Cornelissen\n",
      "\t- Matt Dowle\n",
      "\t- Garrett Grolemund\n",
      "\t- Garrett Grolemund\n",
      "\t- Garrett Grolemund\n",
      "\t- Filip Schouwenaars\n",
      "\t- Gilles Inghelbrecht\n",
      "\t- Nick Carchedi\n",
      "\t- Filip Schouwenaars\n",
      "\t- Filip Schouwenaars\n",
      "\t- Mark Peterson\n"
     ]
    }
   ],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "# Create the Spider class\n",
    "class DCspider( scrapy.Spider ):\n",
    "    name = 'dcspider'\n",
    "    # start_requests method\n",
    "    def start_requests( self ):\n",
    "        yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "    # parse method\n",
    "    def parse( self, response ):\n",
    "        # Create an extracted list of course author names\n",
    "        author_names = response.css( 'p.course-block__author-name::text' ).extract()\n",
    "        # Here we will just return the list of Authors\n",
    "        return author_names\n",
    "\n",
    "def inspect_spider( s ):\n",
    "    news = s()\n",
    "    try:\n",
    "        req = list( news.start_requests() )[0]\n",
    "        url = req.url\n",
    "        html = requests.get( url ).content\n",
    "        response = TextResponse( url = url, body = html, encoding = 'utf-8' )\n",
    "        author_names = req.callback( response )\n",
    "        print( 'You have collected the author names:')\n",
    "        for a in author_names:\n",
    "          print('\\t-', a )\n",
    "    except:\n",
    "        print( 'Oh no! Something went wrong with the code. Keep trying!')\n",
    "  \n",
    "# Inspect the spider\n",
    "inspect_spider( DCspider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One course description you found is: In this introduction to R, you will master the basics of this beautiful open source language, including factors, lists and data frames. With the knowledge gained in this course, you will be ready to undertake your first very own data analysis. With over 2 million users worldwide R is rapidly becoming the leading programming language in statistics and data science. Every year, the number of R users grows by 40% and an increasing number of organizations are using it in their day-to-day activities. Leverage the power of R by completing this free R online course today!\n"
     ]
    }
   ],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "# Create the Spider class\n",
    "class DCdescr( scrapy.Spider ):\n",
    "  name = 'dcdescr'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "  # First parse method\n",
    "  def parse( self, response ):\n",
    "    links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "    # Follow each of the extracted links\n",
    "    for link in links:\n",
    "      yield response.follow( url = link, callback = self.parse_descr )\n",
    "      \n",
    "  # Second parsing method\n",
    "  def parse_descr( self, response ):\n",
    "    # Extract course description\n",
    "    course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "    # For now, just yield the course description\n",
    "    yield course_descr\n",
    "    \n",
    "def inspect_spider( s ):\n",
    "  news = s()\n",
    "  try:\n",
    "    req1 = list( news.start_requests() )[0]\n",
    "    html1 = requests.get( req1.url ).content\n",
    "    response1 = TextResponse( url = req1.url, body = html1, encoding = 'utf-8' )\n",
    "    req2 = list( news.parse( response1 ) )[0]\n",
    "    html2 = requests.get( req2.url ).content\n",
    "    response2 = TextResponse( url = req2.url, body = html2, encoding = 'utf-8' )\n",
    "    for d in news.parse_descr( response2 ):\n",
    "      print(\"One course description you found is:\", d )\n",
    "      break\n",
    "  except:\n",
    "    print(\"Oh no! Something is wrong with the code. Keep trying!\")\n",
    "\n",
    "\n",
    "# Inspect the spider\n",
    "inspect_spider( DCdescr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone\n",
    "\n",
    "### Time to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-18 06:21:09 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\n",
      "2019-05-18 06:21:10 [scrapy.utils.log] INFO: Versions: lxml 4.3.3.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) - [GCC 7.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.5, Platform Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-debian-buster-sid\n",
      "2019-05-18 06:21:10 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2019-05-18 06:21:10 [scrapy.extensions.telnet] INFO: Telnet Password: 0f6bee4724cf8589\n",
      "2019-05-18 06:21:10 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-05-18 06:21:10 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-05-18 06:21:10 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-05-18 06:21:10 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-05-18 06:21:10 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-05-18 06:21:10 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-05-18 06:21:10 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-05-18 06:21:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
      "2019-05-18 06:21:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/ffbc79c0169a150a45a0b503bd19662cb4d44790/free-introduction-to-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:21:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/77b65bcd1c82bf793e2de583c861a077dcec2246/ggvis-data-visualization-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:21:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/60f4cecb02a7e8e78c74643f095e3c913348da9b/dplyr-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:21:14 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://assets.datacamp.com/courses/predicting-customer-churn-in-python> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:21:14 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://assets.datacamp.com/courses/predicting-customer-churn-in-python>: HTTP status code is not handled or not allowed\n",
      "2019-05-18 06:21:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/537eff88062cdcedb77b51d1622aa7675e2baf21/data-table-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:21:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:21:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/6d40286d4abe480763ff8e8ac2246c01861f8c27/intermediate-r-practice> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:21:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/c38792ee5ab59361c958c1b0b4453aa1385acd10/cleaning-data-in-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:21:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/5938f64803ca4a1275f63bf68ac3d300cd9f1c4a/intro-to-python-for-data-science> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:21:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/97b54f43a3d96a7c35defb3c757ecf5471152941/intermediate-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:21:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/9f9f9be002e7c66df8c1733b8796943fa77b2236/introduction-to-machine-learning-with-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:21:14 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-05-18 06:21:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 5215,\n",
      " 'downloader/request_count': 12,\n",
      " 'downloader/request_method_count/GET': 12,\n",
      " 'downloader/response_bytes': 2463103,\n",
      " 'downloader/response_count': 12,\n",
      " 'downloader/response_status_count/200': 11,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 5, 18, 6, 21, 14, 854720),\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/403': 1,\n",
      " 'log_count/DEBUG': 12,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 66940928,\n",
      " 'memusage/startup': 66940928,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 12,\n",
      " 'scheduler/dequeued': 12,\n",
      " 'scheduler/dequeued/memory': 12,\n",
      " 'scheduler/enqueued': 12,\n",
      " 'scheduler/enqueued/memory': 12,\n",
      " 'start_time': datetime.datetime(2019, 5, 18, 6, 21, 10, 181065)}\n",
      "2019-05-18 06:21:14 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A preview of DataCamp Courses:\n",
      "---------------------------------------\n",
      "\n",
      "TITLE: Introduction to R\n",
      "\tChapter 1: Intro to basics\n",
      "\tChapter 2: Vectors\n",
      "\tChapter 3: Matrices\n",
      "\tChapter 4: Factors\n",
      "\tChapter 5: Data frames\n",
      "\tChapter 6: Lists\n",
      "\n",
      "TITLE: Data Visualization in R with ggvis\n",
      "\tChapter 1: The Grammar of Graphics\n",
      "\tChapter 2: Transformations\n",
      "\tChapter 3: Customizing Axes, Legends, and Scales\n",
      "\tChapter 4: Lines and Syntax\n",
      "\tChapter 5: Interactivity and Layers\n",
      "\tChapter 6: The Grammar of Graphics\n",
      "\tChapter 7: Lines and Syntax\n",
      "\tChapter 8: Transformations\n",
      "\tChapter 9: Interactivity and Layers\n",
      "\tChapter 10: Customizing Axes, Legends, and Scales\n",
      "\n",
      "TITLE: Data Manipulation in R with dplyr\n",
      "\tChapter 1: Introduction to dplyr and tbls\n",
      "\tChapter 2: Filter and arrange\n",
      "\tChapter 3: Group_by and working with databases\n",
      "\tChapter 4: Select and mutate\n",
      "\tChapter 5: Summarize and the pipe operator\n",
      "\tChapter 6: Introduction to dplyr and tbls\n",
      "\tChapter 7: Select and mutate\n",
      "\tChapter 8: Filter and arrange\n",
      "\tChapter 9: Summarize and the pipe operator\n",
      "\tChapter 10: Group_by and working with databases\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    ch_titles = response.css('h4.chapter__title::text')\n",
    "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "    \n",
    "def previewCourses( dc_dict, n = 3 ):\n",
    "  crs_titles = list( dc_dict.keys() )\n",
    "  print( \"A preview of DataCamp Courses:\")\n",
    "  print(\"---------------------------------------\\n\")\n",
    "  for t in crs_titles[:n]:\n",
    "    print( \"TITLE: %s\" % t)\n",
    "    for i,ct in enumerate(dc_dict[t]):\n",
    "      print(\"\\tChapter %d: %s\" % (i+1,ct) )\n",
    "    print(\"\")\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataCamp Descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-18 06:28:48 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\n",
      "2019-05-18 06:28:48 [scrapy.utils.log] INFO: Versions: lxml 4.3.3.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) - [GCC 7.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.5, Platform Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-debian-buster-sid\n",
      "2019-05-18 06:28:48 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2019-05-18 06:28:48 [scrapy.extensions.telnet] INFO: Telnet Password: 052f70df5cf92350\n",
      "2019-05-18 06:28:49 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-05-18 06:28:49 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-05-18 06:28:49 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-05-18 06:28:49 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-05-18 06:28:49 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-05-18 06:28:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-05-18 06:28:49 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-05-18 06:28:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
      "2019-05-18 06:28:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/ffbc79c0169a150a45a0b503bd19662cb4d44790/free-introduction-to-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://assets.datacamp.com/production/repositories/2560/datasets/ffbc79c0169a150a45a0b503bd19662cb4d44790/free-introduction-to-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\n",
      "    current.result = callback(current.result, *args, **kw)\n",
      "  File \"<ipython-input-1-4df48489b3ee>\", line 31, in parse_pages\n",
      "    crs_descr = response.css( ____ )\n",
      "NameError: name '____' is not defined\n",
      "2019-05-18 06:28:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/77b65bcd1c82bf793e2de583c861a077dcec2246/ggvis-data-visualization-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://assets.datacamp.com/production/repositories/2560/datasets/77b65bcd1c82bf793e2de583c861a077dcec2246/ggvis-data-visualization-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\n",
      "    current.result = callback(current.result, *args, **kw)\n",
      "  File \"<ipython-input-1-4df48489b3ee>\", line 31, in parse_pages\n",
      "    crs_descr = response.css( ____ )\n",
      "NameError: name '____' is not defined\n",
      "2019-05-18 06:28:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/60f4cecb02a7e8e78c74643f095e3c913348da9b/dplyr-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:28:52 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://assets.datacamp.com/courses/predicting-customer-churn-in-python> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:28:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://assets.datacamp.com/production/repositories/2560/datasets/60f4cecb02a7e8e78c74643f095e3c913348da9b/dplyr-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\n",
      "    current.result = callback(current.result, *args, **kw)\n",
      "  File \"<ipython-input-1-4df48489b3ee>\", line 31, in parse_pages\n",
      "    crs_descr = response.css( ____ )\n",
      "NameError: name '____' is not defined\n",
      "2019-05-18 06:28:53 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://assets.datacamp.com/courses/predicting-customer-churn-in-python>: HTTP status code is not handled or not allowed\n",
      "2019-05-18 06:28:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/537eff88062cdcedb77b51d1622aa7675e2baf21/data-table-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:28:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:28:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://assets.datacamp.com/production/repositories/2560/datasets/537eff88062cdcedb77b51d1622aa7675e2baf21/data-table-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\n",
      "    current.result = callback(current.result, *args, **kw)\n",
      "  File \"<ipython-input-1-4df48489b3ee>\", line 31, in parse_pages\n",
      "    crs_descr = response.css( ____ )\n",
      "NameError: name '____' is not defined\n",
      "2019-05-18 06:28:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/6d40286d4abe480763ff8e8ac2246c01861f8c27/intermediate-r-practice> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:28:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\n",
      "    current.result = callback(current.result, *args, **kw)\n",
      "  File \"<ipython-input-1-4df48489b3ee>\", line 31, in parse_pages\n",
      "    crs_descr = response.css( ____ )\n",
      "NameError: name '____' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-18 06:28:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/5938f64803ca4a1275f63bf68ac3d300cd9f1c4a/intro-to-python-for-data-science> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:28:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://assets.datacamp.com/production/repositories/2560/datasets/6d40286d4abe480763ff8e8ac2246c01861f8c27/intermediate-r-practice> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\n",
      "    current.result = callback(current.result, *args, **kw)\n",
      "  File \"<ipython-input-1-4df48489b3ee>\", line 31, in parse_pages\n",
      "    crs_descr = response.css( ____ )\n",
      "NameError: name '____' is not defined\n",
      "2019-05-18 06:28:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/c38792ee5ab59361c958c1b0b4453aa1385acd10/cleaning-data-in-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:28:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/97b54f43a3d96a7c35defb3c757ecf5471152941/intermediate-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:28:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/9f9f9be002e7c66df8c1733b8796943fa77b2236/introduction-to-machine-learning-with-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2019-05-18 06:28:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://assets.datacamp.com/production/repositories/2560/datasets/5938f64803ca4a1275f63bf68ac3d300cd9f1c4a/intro-to-python-for-data-science> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\n",
      "    current.result = callback(current.result, *args, **kw)\n",
      "  File \"<ipython-input-1-4df48489b3ee>\", line 31, in parse_pages\n",
      "    crs_descr = response.css( ____ )\n",
      "NameError: name '____' is not defined\n",
      "2019-05-18 06:28:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://assets.datacamp.com/production/repositories/2560/datasets/c38792ee5ab59361c958c1b0b4453aa1385acd10/cleaning-data-in-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\n",
      "    current.result = callback(current.result, *args, **kw)\n",
      "  File \"<ipython-input-1-4df48489b3ee>\", line 31, in parse_pages\n",
      "    crs_descr = response.css( ____ )\n",
      "NameError: name '____' is not defined\n",
      "2019-05-18 06:28:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://assets.datacamp.com/production/repositories/2560/datasets/97b54f43a3d96a7c35defb3c757ecf5471152941/intermediate-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\n",
      "    current.result = callback(current.result, *args, **kw)\n",
      "  File \"<ipython-input-1-4df48489b3ee>\", line 31, in parse_pages\n",
      "    crs_descr = response.css( ____ )\n",
      "NameError: name '____' is not defined\n",
      "2019-05-18 06:28:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://assets.datacamp.com/production/repositories/2560/datasets/9f9f9be002e7c66df8c1733b8796943fa77b2236/introduction-to-machine-learning-with-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/twisted/internet/defer.py\", line 654, in _runCallbacks\n",
      "    current.result = callback(current.result, *args, **kw)\n",
      "  File \"<ipython-input-1-4df48489b3ee>\", line 31, in parse_pages\n",
      "    crs_descr = response.css( ____ )\n",
      "NameError: name '____' is not defined\n",
      "2019-05-18 06:28:53 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-05-18 06:28:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 5215,\n",
      " 'downloader/request_count': 12,\n",
      " 'downloader/request_method_count/GET': 12,\n",
      " 'downloader/response_bytes': 2463104,\n",
      " 'downloader/response_count': 12,\n",
      " 'downloader/response_status_count/200': 11,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 5, 18, 6, 28, 53, 894386),\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/403': 1,\n",
      " 'log_count/DEBUG': 12,\n",
      " 'log_count/ERROR': 10,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 69046272,\n",
      " 'memusage/startup': 69046272,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 12,\n",
      " 'scheduler/dequeued': 12,\n",
      " 'scheduler/dequeued/memory': 12,\n",
      " 'scheduler/enqueued': 12,\n",
      " 'scheduler/enqueued/memory': 12,\n",
      " 'spider_exceptions/NameError': 10,\n",
      " 'start_time': datetime.datetime(2019, 5, 18, 6, 28, 49, 159886)}\n",
      "2019-05-18 06:28:53 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A preview of DataCamp Courses:\n",
      "---------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    # Create a SelectorList of the course titles text\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    # Extract the text and strip it clean\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    # Create a SelectorList of course descriptions text\n",
    "    crs_descr = response.css( ____ )\n",
    "    # Extract the text and strip it clean\n",
    "    crs_descr_ext = crs_descr.extract_first().strip()\n",
    "    # Fill in the dictionary\n",
    "    dc_dict[crs_title_ext] = crs_descr_ext\n",
    "    \n",
    "def previewCourses( dc_dict, n = 1 ):\n",
    "  crs_titles = list( dc_dict.keys() )\n",
    "  print( \"A preview of DataCamp Courses:\")\n",
    "  print(\"---------------------------------------\\n\")\n",
    "  for t in crs_titles[:n]:\n",
    "    print( \"TITLE: %s\" % t)\n",
    "    print(\"\\tDescription: %s\" % dc_dict[t] )\n",
    "    print(\"\")\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capstone Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.http import TextResponse\n",
    "import requests\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "response = requests.get(url_short)\n",
    "response = TextResponse(body=response.content, url=url_short)\n",
    "\n",
    "# parse method\n",
    "def parse(self, response):\n",
    "  # Extracted course titles\n",
    "  crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "  # Extracted course descriptions\n",
    "  crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "  # Fill in the dictionary\n",
    "  for crs_title, crs_descr in zip(crs_titles, crs_descrs):\n",
    "    dc_dict[crs_title] = crs_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-18 06:48:04 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\n",
      "2019-05-18 06:48:04 [scrapy.utils.log] INFO: Versions: lxml 4.3.3.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) - [GCC 7.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.5, Platform Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-debian-buster-sid\n",
      "2019-05-18 06:48:04 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2019-05-18 06:48:04 [scrapy.extensions.telnet] INFO: Telnet Password: 8417b1c7ad659f28\n",
      "2019-05-18 06:48:04 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-05-18 06:48:04 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-05-18 06:48:04 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-05-18 06:48:04 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-05-18 06:48:04 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-05-18 06:48:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-05-18 06:48:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-05-18 06:48:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
      "2019-05-18 06:48:06 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-05-18 06:48:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 306,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 171262,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 5, 18, 6, 48, 6, 461979),\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/INFO': 9,\n",
      " 'memusage/max': 74104832,\n",
      " 'memusage/startup': 74104832,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2019, 5, 18, 6, 48, 4, 330183)}\n",
      "2019-05-18 06:48:06 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A preview of DataCamp Courses:\n",
      "---------------------------------------\n",
      "\n",
      "TITLE: Introduction to R\n",
      "\tDESCRIPTION: \n",
      "          Master the basics of data analysis by manipulating common data structures such as vectors, matrices and data frames.\n",
      "        \n",
      "\n",
      "TITLE: Data Analysis in R, the data.table Way\n",
      "\tDESCRIPTION: \n",
      "          Master core concepts in data manipulation such as subsetting, updating, indexing and joining your data using data.table.\n",
      "        \n",
      "\n",
      "TITLE: Data Manipulation in R with dplyr\n",
      "\tDESCRIPTION: \n",
      "          Master techniques for data manipulation using the select, mutate, filter, arrange, and summarise functions in dplyr.\n",
      "        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = 'yourspider'\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short, callback = self.parse)\n",
    "      \n",
    "  def parse(self, response):\n",
    "    # My version of the parser you wrote in the previous part\n",
    "    crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "    crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "    for crs_title, crs_descr in zip(crs_titles, crs_descrs):\n",
    "      dc_dict[crs_title] = crs_descr\n",
    "    \n",
    "def previewCourses( dc_dict, n = 3 ):\n",
    "  parse( self = None, response = response )\n",
    "  crs_titles = list( dc_dict.keys() )\n",
    "  print( \"A preview of DataCamp Courses:\")\n",
    "  print(\"---------------------------------------\\n\")\n",
    "  for t in crs_titles[:n]:\n",
    "    print( \"TITLE: %s\" % t)\n",
    "    print( \"\\tDESCRIPTION: %s\" % dc_dict[t] )\n",
    "    print(\"\")\n",
    "    \n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(YourSpider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Scratching and Start Scraping!\n",
    "\n",
    "### Feeding the Machine\n",
    "\n",
    "`Process Data Acquistion` **Access Raw Data --> Parse & Extract**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Skills\n",
    "\n",
    "* **Objective**: Scrape situs web secara komputasi\n",
    "* **Bagaimana**? Kita memutuskan untuk menggunakan scrapy\n",
    "* **Bagaimana**? Kita perlu bekerja dengan:\n",
    "  * `Selector` dan `Response` objects\n",
    "  * Mungkin bahkan membuat Spider\n",
    "* **Bagaimana**? Kita perlu mempelajari Notasi XPath dan CSS Locator\n",
    "* **Bagaimana**? Memahami struktur HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apa yang perlu diketahui?\n",
    "\n",
    "* Struktur HTML\n",
    "* Notasi XPath dan CSS Locator\n",
    "* Cara menggunakan `Selector` dan `Response` objects di `scrapy`\n",
    "* Cara mengatur spider\n",
    "* Cara scrape situs web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
