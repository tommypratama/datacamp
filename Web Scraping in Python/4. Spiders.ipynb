{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spiders\n",
    "\n",
    "Pelajari cara membuat web crawlers dengan scrapy. Scrapy spiders ini akan melakukan proses crawl pada web melalui beberapa halaman, mengikuti tautan untuk scrape setiap halaman secara otomatis sesuai dengan prosedur yang telah kita pelajari di bab-bab sebelumnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your First Spider\n",
    "\n",
    "* Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weaving the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCspider( scrapy.Spider ):\n",
    "    \n",
    "    name = 'dc_spider'\n",
    "    \n",
    "    def start_requests( self ):\n",
    "        urls = [ 'https://www.datacamp.com/courses/all' ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request( url = url, callback = self.parse )\n",
    "\n",
    "    def parse( self, response ):\n",
    "        # simple example: write out the html\n",
    "        html_file = 'datasets/DC_courses.html'\n",
    "        with open( html_file, 'wb' ) as fout:\n",
    "            fout.write( response.body )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perlu memiliki fungsi yang disebut `start_requests`\n",
    "* Harus memiliki setidaknya satu fungsi pengurai/parser untuk menangani kode HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Skinny on start_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_requests( self ):\n",
    "    urls = ['https://www.datacamp.com/courses/all']\n",
    "    for url in urls:\n",
    "        yield scrapy.Request( url = url, callback = self.parse )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `scrapy.Request` di sini akan ada dalam variabel response untuk kita\n",
    "* Argumen `url` memberi tahu kami situs mana yang ingin di scrape\n",
    "* Argumen `callback` memberi tahu kita ke mana harus mengirim variabel respons untuk diproses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inheriting the Spider\n",
    "\n",
    "Saat mempelajari tentang `scrapy` spiders, kami melihat bahwa bagian utama dari kode yang disesuaikan adalah `class` untuk spider. Untuk membantu membangun keakraban class, Anda akan menyelesaikan sepotong kode pendek untuk menyelesaikan model mainan kode class spider. Kami telah menghilangkan kode yang benar-benar menjalankan spider, hanya menyertakan bagian yang diperlukan untuk membuat class.\n",
    "\n",
    "Seperti disebutkan dalam pelajaran, `class` merupakan sekumpulan variabel terkait dan fungsi yang ditempatkan bersama. Terkadang satu class suka menggunakan metode dari class lain, jadi kami akan mewarisi metode dari class yang berbeda. Itulah yang kami lakukan di class spider.\n",
    "\n",
    "Kami menulis fungsi `inspect_class` untuk melihat class Anda setelah selesai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your spider class name is: your_spider\n",
      "It seems you have inherited methods from scrapy.Spider -- NICE!\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "    name = \"your_spider\"\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        pass\n",
    "    # parse method\n",
    "    def parse(self, response):\n",
    "        pass\n",
    "\n",
    "def inspect_class(c):\n",
    "    newc = c()\n",
    "    meths = dir(newc)\n",
    "    if 'name' in meths:\n",
    "        print(\"Your spider class name is:\", newc.name)\n",
    "    if 'from_crawler' in meths:\n",
    "        print(\"It seems you have inherited methods from scrapy.Spider -- NICE!\")\n",
    "    else:\n",
    "        print(\"Oh no! It doesn't seem that you are inheriting the methods from scrapy.Spider!!\")\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class(YourSpider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hurl the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The start_requests method yields the following urls:\n",
      "\t- https://www.datacamp.com\n",
      "\t- https://scrapy.org\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "    name = \"your_spider\"\n",
    "    # start_requests method\n",
    "    def start_requests( self ):\n",
    "    urls = [\"https://www.datacamp.com\", \"https://scrapy.org\"]\n",
    "    for url in urls:\n",
    "        yield url\n",
    "    \n",
    "    # parse method\n",
    "    def parse( self, response ):\n",
    "        pass\n",
    "\n",
    "def inspect_class( c ):\n",
    "    newc = c()\n",
    "    meths = dir( newc )\n",
    "    if 'start_requests' in meths:\n",
    "        print( \"The start_requests method yields the following urls:\" )\n",
    "    for u in newc.start_requests():\n",
    "        print(  \"\\t-\", u )\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Referencing is Classy\n",
    "\n",
    "Anda mungkin telah memperhatikan bahwa di dalam class spider, kami selalu memasukkan argumen `self` dalam metode `start_requests` dan `parse`.\n",
    "\n",
    "Ini memungkinkan kita untuk referensi antar metode di dalam class. Yaitu, jika kita ingin merujuk ke metode `parse` dalam metode `start_requests`, kita perlu menulis `self.parse` daripada hanya `parse`; apa yang dilakukan `self` adalah memberi tahu kode: \"Lihat di class yang sama dengan `start_requests` untuk metode yang disebut `parse` agar digunakan.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling start_requests in YourSpider prints out: Hello World!\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "    name = \"your_spider\"\n",
    "    # start_requests method\n",
    "    def start_requests( self ):\n",
    "        self.print_msg( \"Hello World!\" )\n",
    "    # parse method\n",
    "    def parse( self, response ):\n",
    "        pass\n",
    "    # print_msg method\n",
    "    def print_msg( self, msg ):\n",
    "        print( \"Calling start_requests in YourSpider prints out:\", msg )\n",
    "\n",
    "def inspect_class( c ):\n",
    "    newc = c()\n",
    "    try:\n",
    "        newc.start_requests()\n",
    "    except:\n",
    "        print( \"Oh No! Something is wrong with the code! Keep trying.\" )\n",
    "\n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with Start Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The url you would scrape is: https://www.datacamp.com\n",
      "The name of the callback method you called is: parse\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "    name = \"your_spider\"\n",
    "    # start_requests method\n",
    "    def start_requests( self ):\n",
    "        yield scrapy.Request( url = \"https://www.datacamp.com\", callback = self.parse )\n",
    "    # parse method\n",
    "    def parse( self, response ):\n",
    "        pass\n",
    "\n",
    "def inspect_class( c ):\n",
    "    newc = c()\n",
    "    try:\n",
    "        y = list( newc.start_requests() )\n",
    "        first_yield = y[0]\n",
    "        print( \"The url you would scrape is:\", first_yield.url )\n",
    "        cb = first_yield.callback\n",
    "        print( \"The name of the callback method you called is:\", cb.__name__ )\n",
    "    except:\n",
    "        print( \"Oh No! Something is wrong with the code! Keep trying.\" )\n",
    "    \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and Crawl\n",
    "\n",
    "### Pen Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have collected the author names:\n",
      "\t- Jonathan Cornelissen\n",
      "\t- Matt Dowle\n",
      "\t- Garrett Grolemund\n",
      "\t- Garrett Grolemund\n",
      "\t- Garrett Grolemund\n",
      "\t- Filip Schouwenaars\n",
      "\t- Gilles Inghelbrecht\n",
      "\t- Nick Carchedi\n",
      "\t- Filip Schouwenaars\n",
      "\t- Filip Schouwenaars\n",
      "\t- Mark Peterson\n"
     ]
    }
   ],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "# Create the Spider class\n",
    "class DCspider( scrapy.Spider ):\n",
    "    name = 'dcspider'\n",
    "    # start_requests method\n",
    "    def start_requests( self ):\n",
    "        yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "    # parse method\n",
    "    def parse( self, response ):\n",
    "        # Create an extracted list of course author names\n",
    "        author_names = response.css( 'p.course-block__author-name::text' ).extract()\n",
    "        # Here we will just return the list of Authors\n",
    "        return author_names\n",
    "\n",
    "def inspect_spider( s ):\n",
    "    news = s()\n",
    "    try:\n",
    "        req = list( news.start_requests() )[0]\n",
    "        url = req.url\n",
    "        html = requests.get( url ).content\n",
    "        response = TextResponse( url = url, body = html, encoding = 'utf-8' )\n",
    "        author_names = req.callback( response )\n",
    "        print( 'You have collected the author names:')\n",
    "        for a in author_names:\n",
    "            print('\\t-', a )\n",
    "    except:\n",
    "        print( 'Oh no! Something went wrong with the code. Keep trying!')\n",
    "        \n",
    "# Inspect the spider\n",
    "inspect_spider( DCspider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One course description you found is: In this introduction to R, you will master the basics of this beautiful open source language, including factors, lists and data frames. With the knowledge gained in this course, you will be ready to undertake your first very own data analysis. With over 2 million users worldwide R is rapidly becoming the leading programming language in statistics and data science. Every year, the number of R users grows by 40% and an increasing number of organizations are using it in their day-to-day activities. Leverage the power of R by completing this free R online course today!\n"
     ]
    }
   ],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "\n",
    "# Create the Spider class\n",
    "class DCdescr( scrapy.Spider ):\n",
    "    name = 'dcdescr'\n",
    "    # start_requests method\n",
    "    def start_requests( self ):\n",
    "        yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "    # First parse method\n",
    "    def parse( self, response ):\n",
    "        links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "        # Follow each of the extracted links\n",
    "        for link in links:\n",
    "            yield response.follow( url = link, callback = self.parse_descr )\n",
    "      \n",
    "    # Second parsing method\n",
    "    def parse_descr( self, response ):\n",
    "        # Extract course description\n",
    "        course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "        # For now, just yield the course description\n",
    "        yield course_descr\n",
    "    \n",
    "def inspect_spider( s ):\n",
    "    news = s()\n",
    "    try:\n",
    "        req1 = list( news.start_requests() )[0]\n",
    "        html1 = requests.get( req1.url ).content\n",
    "        response1 = TextResponse( url = req1.url, body = html1, encoding = 'utf-8' )\n",
    "        req2 = list( news.parse( response1 ) )[0]\n",
    "        html2 = requests.get( req2.url ).content\n",
    "        response2 = TextResponse( url = req2.url, body = html2, encoding = 'utf-8' )\n",
    "        for d in news.parse_descr( response2 ):\n",
    "          print(\"One course description you found is:\", d )\n",
    "          break\n",
    "    except:\n",
    "        print(\"Oh no! Something is wrong with the code. Keep trying!\")\n",
    "\n",
    "\n",
    "# Inspect the spider\n",
    "inspect_spider( DCdescr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capstone\n",
    "\n",
    "### Time to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-15 05:24:43 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: scrapybot)\n",
      "2020-02-15 05:24:43 [scrapy.utils.log] INFO: Versions: lxml 4.4.2.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.3 | packaged by conda-forge | (default, Jul  1 2019, 21:52:21) - [GCC 7.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Linux-5.0.0-1029-gcp-x86_64-with-debian-buster-sid\n",
      "2020-02-15 05:24:43 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2020-02-15 05:24:43 [scrapy.extensions.telnet] INFO: Telnet Password: 997c49bc3533869d\n",
      "2020-02-15 05:24:43 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-02-15 05:24:43 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-02-15 05:24:43 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-02-15 05:24:43 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-02-15 05:24:43 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-02-15 05:24:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-02-15 05:24:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-02-15 05:24:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
      "2020-02-15 05:24:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/ffbc79c0169a150a45a0b503bd19662cb4d44790/free-introduction-to-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-02-15 05:24:44 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://assets.datacamp.com/courses/predicting-customer-churn-in-python> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-02-15 05:24:44 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 https://assets.datacamp.com/courses/predicting-customer-churn-in-python>: HTTP status code is not handled or not allowed\n",
      "2020-02-15 05:24:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/844f6aa70cafd81cbc92f344baff641173604229/reporting-with-r-markdown> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-02-15 05:24:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/537eff88062cdcedb77b51d1622aa7675e2baf21/data-table-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-02-15 05:24:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/77b65bcd1c82bf793e2de583c861a077dcec2246/ggvis-data-visualization-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-02-15 05:24:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/60f4cecb02a7e8e78c74643f095e3c913348da9b/dplyr-data-manipulation-r-tutorial> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-02-15 05:24:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/6d40286d4abe480763ff8e8ac2246c01861f8c27/intermediate-r-practice> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-02-15 05:24:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/c38792ee5ab59361c958c1b0b4453aa1385acd10/cleaning-data-in-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-02-15 05:24:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/5938f64803ca4a1275f63bf68ac3d300cd9f1c4a/intro-to-python-for-data-science> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-02-15 05:24:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/9f9f9be002e7c66df8c1733b8796943fa77b2236/introduction-to-machine-learning-with-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-02-15 05:24:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/97b54f43a3d96a7c35defb3c757ecf5471152941/intermediate-r> (referer: https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short)\n",
      "2020-02-15 05:24:47 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-02-15 05:24:47 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 5215,\n",
      " 'downloader/request_count': 12,\n",
      " 'downloader/request_method_count/GET': 12,\n",
      " 'downloader/response_bytes': 2466036,\n",
      " 'downloader/response_count': 12,\n",
      " 'downloader/response_status_count/200': 11,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'elapsed_time_seconds': 3.660063,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 2, 15, 5, 24, 47, 272916),\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/403': 1,\n",
      " 'log_count/DEBUG': 12,\n",
      " 'log_count/INFO': 11,\n",
      " 'memusage/max': 142827520,\n",
      " 'memusage/startup': 142827520,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 12,\n",
      " 'scheduler/dequeued': 12,\n",
      " 'scheduler/dequeued/memory': 12,\n",
      " 'scheduler/enqueued': 12,\n",
      " 'scheduler/enqueued/memory': 12,\n",
      " 'start_time': datetime.datetime(2020, 2, 15, 5, 24, 43, 612853)}\n",
      "2020-02-15 05:24:47 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A preview of DataCamp Courses:\n",
      "---------------------------------------\n",
      "\n",
      "TITLE: Introduction to R\n",
      "\tChapter 1: Intro to basics\n",
      "\tChapter 2: Vectors\n",
      "\tChapter 3: Matrices\n",
      "\tChapter 4: Factors\n",
      "\tChapter 5: Data frames\n",
      "\tChapter 6: Lists\n",
      "\n",
      "TITLE: Reporting with R Markdown\n",
      "\tChapter 1: Authoring R Markdown Reports\n",
      "\tChapter 2: Embedding Code\n",
      "\tChapter 3: Compiling Reports\n",
      "\tChapter 4: Configuring R Markdown (optional)\n",
      "\n",
      "TITLE: Data Analysis in R, the data.table Way\n",
      "\tChapter 1: Data.table novice\n",
      "\tChapter 2: Data.table yeoman\n",
      "\tChapter 3: Data.table expert\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url = url_short, callback = self.parse_front)\n",
    "    \n",
    "    # First parsing method\n",
    "    def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "        yield response.follow(url = url, callback = self.parse_pages)\n",
    "\n",
    "    # Second parsing method\n",
    "    def parse_pages(self, response):\n",
    "        crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "        ch_titles = response.css('h4.chapter__title::text')\n",
    "        ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "        dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "    \n",
    "def previewCourses( dc_dict, n = 3 ):\n",
    "    crs_titles = list( dc_dict.keys() )\n",
    "    print( \"A preview of DataCamp Courses:\")\n",
    "    print(\"---------------------------------------\\n\")\n",
    "    for t in crs_titles[:n]:\n",
    "        print( \"TITLE: %s\" % t)\n",
    "        for i,ct in enumerate(dc_dict[t]):\n",
    "          print(\"\\tChapter %d: %s\" % (i+1,ct) )\n",
    "        print(\"\")\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataCamp Descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url = url_short, callback = self.parse_front)\n",
    "        \n",
    "  # First parsing method\n",
    "    def parse_front(self, response):\n",
    "        course_blocks = response.css('div.course-block')\n",
    "        course_links = course_blocks.xpath('./a/@href')\n",
    "        links_to_follow = course_links.extract()\n",
    "        for url in links_to_follow:\n",
    "            yield response.follow(url = url, callback = self.parse_pages)\n",
    "        \n",
    "    # Second parsing method\n",
    "    def parse_pages(self, response):\n",
    "        # Create a SelectorList of the course titles text\n",
    "        crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "        # Extract the text and strip it clean\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "        # Create a SelectorList of course descriptions text\n",
    "        crs_descr = response.css( 'p.course__description::text' )\n",
    "        # Extract the text and strip it clean\n",
    "        crs_descr_ext = crs_descr.extract_first().strip()\n",
    "        # Fill in the dictionary\n",
    "        dc_dict[crs_title_ext] = crs_descr_ext\n",
    "    \n",
    "    def previewCourses( dc_dict, n = 1 ):\n",
    "        crs_titles = list( dc_dict.keys() )\n",
    "        print( \"A preview of DataCamp Courses:\")\n",
    "        print(\"---------------------------------------\\n\")\n",
    "        for t in crs_titles[:n]:\n",
    "            print( \"TITLE: %s\" % t)\n",
    "            print(\"\\tDescription: %s\" % dc_dict[t] )\n",
    "            print(\"\")\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capstone Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.http import TextResponse\n",
    "import requests\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "response = requests.get(url_short)\n",
    "response = TextResponse(body=response.content, url=url_short)\n",
    "\n",
    "# parse method\n",
    "def parse(self, response):\n",
    "  # Extracted course titles\n",
    "  crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "  # Extracted course descriptions\n",
    "  crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "  # Fill in the dictionary\n",
    "  for crs_title, crs_descr in zip(crs_titles, crs_descrs):\n",
    "    dc_dict[crs_title] = crs_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-18 06:48:04 [scrapy.utils.log] INFO: Scrapy 1.6.0 started (bot: scrapybot)\n",
      "2019-05-18 06:48:04 [scrapy.utils.log] INFO: Versions: lxml 4.3.3.0, libxml2 2.9.9, cssselect 1.0.3, parsel 1.5.1, w3lib 1.20.0, Twisted 19.2.0, Python 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34) - [GCC 7.3.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1b  26 Feb 2019), cryptography 2.5, Platform Linux-3.10.0-957.12.2.el7.x86_64-x86_64-with-debian-buster-sid\n",
      "2019-05-18 06:48:04 [scrapy.crawler] INFO: Overridden settings: {}\n",
      "2019-05-18 06:48:04 [scrapy.extensions.telnet] INFO: Telnet Password: 8417b1c7ad659f28\n",
      "2019-05-18 06:48:04 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2019-05-18 06:48:04 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2019-05-18 06:48:04 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2019-05-18 06:48:04 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2019-05-18 06:48:04 [scrapy.core.engine] INFO: Spider opened\n",
      "2019-05-18 06:48:04 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2019-05-18 06:48:04 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2019-05-18 06:48:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short> (referer: None)\n",
      "2019-05-18 06:48:06 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2019-05-18 06:48:06 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 306,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 171262,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2019, 5, 18, 6, 48, 6, 461979),\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/INFO': 9,\n",
      " 'memusage/max': 74104832,\n",
      " 'memusage/startup': 74104832,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2019, 5, 18, 6, 48, 4, 330183)}\n",
      "2019-05-18 06:48:06 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A preview of DataCamp Courses:\n",
      "---------------------------------------\n",
      "\n",
      "TITLE: Introduction to R\n",
      "\tDESCRIPTION: \n",
      "          Master the basics of data analysis by manipulating common data structures such as vectors, matrices and data frames.\n",
      "        \n",
      "\n",
      "TITLE: Data Analysis in R, the data.table Way\n",
      "\tDESCRIPTION: \n",
      "          Master core concepts in data manipulation such as subsetting, updating, indexing and joining your data using data.table.\n",
      "        \n",
      "\n",
      "TITLE: Data Manipulation in R with dplyr\n",
      "\tDESCRIPTION: \n",
      "          Master techniques for data manipulation using the select, mutate, filter, arrange, and summarise functions in dplyr.\n",
      "        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "    name = 'yourspider'\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url = url_short, callback = self.parse)\n",
    "      \n",
    "    def parse(self, response):\n",
    "    # My version of the parser you wrote in the previous part\n",
    "    crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "    crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "    for crs_title, crs_descr in zip(crs_titles, crs_descrs):\n",
    "        dc_dict[crs_title] = crs_descr\n",
    "    \n",
    "def previewCourses( dc_dict, n = 3 ):\n",
    "    parse( self = None, response = response )\n",
    "    crs_titles = list( dc_dict.keys() )\n",
    "    print( \"A preview of DataCamp Courses:\")\n",
    "    print(\"---------------------------------------\\n\")\n",
    "    for t in crs_titles[:n]:\n",
    "        print( \"TITLE: %s\" % t)\n",
    "        print( \"\\tDESCRIPTION: %s\" % dc_dict[t] )\n",
    "        print(\"\")\n",
    "    \n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(YourSpider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Scratching and Start Scraping!\n",
    "\n",
    "### Feeding the Machine\n",
    "\n",
    "`Process Data Acquistion` **Access Raw Data --> Parse & Extract**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Skills\n",
    "\n",
    "* **Objective**: Scrape situs web secara komputasi\n",
    "* **Bagaimana**? Kita memutuskan untuk menggunakan scrapy\n",
    "* **Bagaimana**? Kita perlu bekerja dengan:\n",
    "  * `Selector` dan `Response` objects\n",
    "  * Mungkin bahkan membuat Spider\n",
    "* **Bagaimana**? Kita perlu mempelajari Notasi XPath dan CSS Locator\n",
    "* **Bagaimana**? Memahami struktur HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apa yang perlu diketahui?\n",
    "\n",
    "* Struktur HTML\n",
    "* Notasi XPath dan CSS Locator\n",
    "* Cara menggunakan `Selector` dan `Response` objects di `scrapy`\n",
    "* Cara mengatur spider\n",
    "* Cara scrape situs web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
